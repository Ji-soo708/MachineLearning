{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbltlYE_x4ZX"
      },
      "source": [
        "<h2>개인 구글 드라이브와 colab 연동</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6q5Z9pNGA0M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a476ee5c-3df1-4b55-beef-2dfc80523efa"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlXCd1UpzDLL"
      },
      "source": [
        "<h2>\"SMSSpamCollection\" 데이터를 읽고 문장과 정답을 분리하여 각 리스트에 저장</h2>\n",
        "\n",
        "<pre>\n",
        "<b>1. 데이터의 형태(SMSSpamCollection)</b>\n",
        "  라벨(스팸 또는 햄) \\t(tab) 문장 \n",
        "  \n",
        "  위와 같은 형태로 저장되어 있음\n",
        "  \n",
        "  예시)\n",
        "    ham\\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
        "    spam\\tCustomer service annoncement. You have a New Years delivery waiting for you. Please call 07046744435 now to arrange delivery\n",
        "    ...\n",
        "  \n",
        "  따라서 입력 데이터를 읽고 \\t을 기준으로 입력 문장을 분리한 후에 문장과 라벨을 각각 x_data, y_data 리스트에 저장\n",
        "  \n",
        "<b>2. 입력 데이터 전체를 사용하지 않고 100개만 추출해서 사용</b>\n",
        "\n",
        "<b>3. x_data, y_data 형태</b>\n",
        "  x_data = [ 문장1, 문장2, 문장3, ... 문장100]\n",
        "  y_data = [ 문장1의 라벨, 문장2의 라벨, 문장3의 라벨, ... 문장100의 라벨]\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUuFzwfHGFrq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f7d276e-7b60-4813-f128-7e452a469315"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "file_path = \"/gdrive/My Drive/colab/svm/SMSSpamCollection.dat\"\n",
        "\n",
        "# 파일 읽기\n",
        "x_data, y_data = [], [] #x:데이터 y:레이블\n",
        "with open(file_path,'r',encoding='utf8') as inFile:\n",
        "  lines = inFile.readlines()\n",
        "  \n",
        "lines = lines[:100]  #100개만 읽어옴\n",
        "  \n",
        "  #라인 하나씩 가져와서 레이블과 데이터를 탭으로 분리, 뒤쪽엔 문장(sentence), 앞에는 정답레이블(label)\n",
        "for line in lines:\n",
        "  line = line.strip().split('\\t')\n",
        "  sentence, label = line[1], line[0]\n",
        "  x_data.append(sentence)\n",
        "  y_data.append(label)\n",
        "  \n",
        "print(\"x_data의 개수 : \" + str(len(x_data)))\n",
        "print(\"y_data의 개수 : \" + str(len(y_data)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_data의 개수 : 100\n",
            "y_data의 개수 : 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LPNC4jTS-CBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3R9OE2Cp1jRX"
      },
      "source": [
        "<h2>Tokenizer 라이브러리를 사용하여 입력 문장을 index로 치환</h2>\n",
        "\n",
        "<pre>\n",
        "<b>1. tokenizer.fit_on_texts(data) 함수를 이용하여 각 단어를 index로 치환하기 위한 딕셔너리 생성</b>\n",
        "   생성된 딕셔너리는 tokenizer 객체 안에 저장됨\n",
        "  \n",
        "  tokenizer.fit_on_texts(data)\n",
        "  args\n",
        "    data : 문자열 element를 가지고 있는 리스트\n",
        "  return\n",
        "    X\n",
        "    \n",
        "  딕셔너리 예시)\n",
        "    {'to': 1, 'i': 2, 'you': 3, 'a': 4, 'the': 5, 'and': 6, 'for': 7 ... }\n",
        "    \n",
        "<b>2. tokenizer.texts_to_sequences(data) 함수를 이용하여 문장 안에 있는 단어들을 index로 치환</b>\n",
        "\n",
        "  tokenizer.texts_to_sequence(data)\n",
        "  args\n",
        "    data : 문자열 element를 가지고 있는 리스트\n",
        "  return : \n",
        "    indexing 된 리스트\n",
        "    \n",
        "  indexing 예시)\n",
        "    x_data indexing 하기 전 : Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
        "    x_data indexing 하기 후 : [38, 93, 239, 240, 241, 242, 53, 11, 243, 72, 94, 244, 245, 126, 246, 247, 73, 74, 248, 127]\n",
        "    y_data indexing 하기 전 : ham\n",
        "    y_data indexing 하기 후 : 1\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEagO2Q0GOBM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fbca4be-94a6-4963-bb6e-edb6f0ddd2a5"
      },
      "source": [
        "# decision tree와 달리 숫자로 매핑해야함\n",
        "# 데이터 변환 (문자열 -> 숫자)\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer() #스페이스 단위로 끊으는 것(어절단위)\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "stopwords_list=nltk.corpus.stopwords.words(\"english\")\n",
        "\n",
        "print(\"영어 불용어 개수\",len(nltk.corpus.stopwords.words('english')))\n",
        "print(nltk.corpus.stopwords.words(\"english\"[:10]))\n",
        "\n",
        "#여기서 x_data와 불용어 비교할 코드 쓰기\n",
        "\n",
        "fin_xdata=[]\n",
        "\n",
        "for i in x_data:\n",
        "  corpus=[] #말뭉치 리스트\n",
        "  i=i.lower() #불용어들은 다 소문자로 되어있어서 말뭉치 또한 소문자 처리를 해 정확도를 높였다.\n",
        "  #소문자 처리전: 50%\n",
        "  #소문자 처리후: 60%\n",
        "  corpus=i.split() # 말뭉치로 분리\n",
        "\n",
        "  for stopwords_i in stopwords_list:\n",
        "    while stopwords_i in corpus:\n",
        "      corpus.remove(stopwords_i) #말뭉치에서 불용어 제거\n",
        "\n",
        "  #말뭉치들을 다시 처음처럼 문장으로 합침\n",
        "  fin_sen=\" \".join(corpus)\n",
        "  fin_xdata.append(fin_sen)\n",
        "\n",
        "\n",
        "#x_data를 불용어를 제거한 fin_xdata로 갱신\n",
        "x_data=fin_xdata\n",
        "\n",
        "# spam, ham 라벨을 대응하는 index로 치환하기위한 딕셔너리\n",
        "label2index_dict = {'spam':0, 'ham':1}\n",
        "\n",
        "# 문자열이었는데 숫자로 바꾼 데이터를 넣을 거임\n",
        "# indexing 한 데이터를 넣을 리스트 선언\n",
        "# 초기화\n",
        "indexing_x_data, indexing_y_data = [], []\n",
        "\n",
        "# y_data에 있는 각 문장의 단어들을 대응하는 index로 치환하고 그 결과값을 indexing_y_data에 저장\n",
        "for label in y_data:\n",
        "  indexing_y_data.append(label2index_dict[label])\n",
        "\n",
        "# x_data를 사용하여 딕셔너리 생성\n",
        "tokenizer.fit_on_texts(x_data)                   \n",
        "\n",
        "# x_data에 있는 각 문장의 단어들을 대응하는 index로 치환하고 그 결과값을 indexing_x_data에 저장\n",
        "indexing_x_data = tokenizer.texts_to_sequences(x_data)    \n",
        "\n",
        "print(\"x_data indexing 하기 전 : \" + str(x_data[0]))\n",
        "print(\"x_data indexing 하기 후 : \" + str(indexing_x_data[0]))\n",
        "print(\"y_data indexing 하기 전 : \" + str(y_data[0]))\n",
        "print(\"y_data indexing 하기 후 : \" + str(indexing_y_data[0]))\n",
        "\n",
        "##train 할 데이터 다 만듦"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "영어 불용어 개수 179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "x_data indexing 하기 전 : go jurong point, crazy.. available bugis n great world la e buffet... cine got amore wat...\n",
            "x_data indexing 하기 후 : [8, 177, 178, 179, 180, 181, 29, 44, 182, 183, 71, 184, 185, 30, 186, 72]\n",
            "y_data indexing 하기 전 : ham\n",
            "y_data indexing 하기 후 : 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iUL8La1iqtd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kgJMF17Q-Dc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nN5GGyQI_IpT"
      },
      "source": [
        "<h2>SVM 모델 학습</h2>\n",
        "\n",
        "<pre>\n",
        "<b>1. 데이터의 문장 길이를 고정된 길이로 변환</b>\n",
        "  예제 코드에서는 60으로 맞추어 변환\n",
        "  \n",
        "  문장 길이가 60 초과인 경우 뒷 부분 제거\n",
        "  문장 길이가 60 미만인 경우 나머지 부분을 0으로 채움\n",
        "  \n",
        "  예시)\n",
        "    문장 길이를 5로 맞추고자 할 경우\n",
        "    \n",
        "    문장 길이가 5보다 큰 경우\n",
        "    문장 : 나는 어제 집에서 저녁으로 김치찌개를 먹었다, indexing_문장 : 38, 93, 239, 240, 241, 242\n",
        "    38, 93, 239, 240, 241, 242 -> 38, 93, 239, 240, 241\n",
        "    \n",
        "    문장 길이가 5보다 작은 경우\n",
        "    문장 : 나는 김치찌개를 좋아해, indexing_문장 : 74, 248, 127\n",
        "    74, 248, 127 -> 74, 248, 127, 0, 0\n",
        "    \n",
        "<b>2. 입력 데이터를 9 대 1 비율로 나누어 학습, 평가에 사용</b>\n",
        "  train_x = [ 문장1, 문장2, 문장3, ... 문장90]\n",
        "  train_y = [ 문장1의 라벨, 문장2의 라벨, 문장3의 라벨, ... 문장90의 라벨]\n",
        "  test_x = [ 문장91, 문장92, 문장93, ... 문장100]\n",
        "  test_y = [ 문장91의 라벨, 문장92의 라벨, 문장93의 라벨, ... 문장100의 라벨]\n",
        "  \n",
        "<b>3. svm.fit(x, y) 함수를 사용하여 SVM 모델 학습</b>\n",
        "  svm.fit(x, y)\n",
        "  args\n",
        "    x : indexing 된 문장들이 있는 리스트\n",
        "    y : x의 각 문장에 대응하는 라벨이 있는 리스트\n",
        "  return : \n",
        "    X\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYNBrDnzGO-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5e8a46d-2507-42ee-ad52-99539260cbe2"
      },
      "source": [
        "# svm중 SVC(ham이냐 spam이냐 분류함)만 가져올 거다\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# svm에서는 단어 하나하나가 차원축임\n",
        "# a라는 단어가 3개, b라는 단어가 2개면 (3,2)\n",
        "# list니까 1차원?? 아니다\n",
        "\n",
        "# 문장의 길이를 max_length으로 맞춰 변환\n",
        "max_length = 60\n",
        "for index in range(len(indexing_x_data)):\n",
        "  length = len(indexing_x_data[index])\n",
        "  \n",
        "  if(length > max_length):\n",
        "    indexing_x_data[index] = indexing_x_data[index][:max_length]\n",
        "  elif(length < max_length):\n",
        "    indexing_x_data[index] = indexing_x_data[index] + [0]*(max_length-length) # 빈 공간은 0으로 채워서 길이를 맞춤\n",
        "    \n",
        "    \n",
        "# 전체 데이터를 9:1의 비율로 나누어 학습 및 평가 데이터로 사용\n",
        "# train set: 90%, test set: 10%\n",
        "number_of_train = int(len(indexing_x_data)*0.9)\n",
        "\n",
        "train_x = indexing_x_data[:number_of_train]\n",
        "train_y = indexing_y_data[:number_of_train]\n",
        "test_x = indexing_x_data[number_of_train:] # 90번째부터 끝까지\n",
        "test_y = indexing_y_data[number_of_train:]\n",
        "\n",
        "print(\"train_x의 개수 : \" + str(len(train_x)))\n",
        "print(\"train_y의 개수 : \" + str(len(train_y)))\n",
        "print(\"test_x의 개수 : \" + str(len(test_x)))\n",
        "print(\"test_y의 개수 : \" + str(len(test_y)))\n",
        "\n",
        "# !중요!\n",
        "# 단어 하나하나가 차원, feature이 되는데 차원이 굉장히 많은 문제는 linear kernel 이 working 잘함\n",
        "# 차원이 작으면 polynomial 가 좋음\n",
        "# C는 soft margin 값 (어느정도 마진까지 허락할지)\n",
        "# 실제로는 clear 하게 나누는 선은 못구하므로 어느정도 에러는 허용해야 함\n",
        "# 마진안에 틀린 것들이 있을텐데 얼마나 허락해줄지\n",
        "# hard margin: 하나도 에러 허용안해줌, soft margin: 어느정도는 에러를 허용해주겠다 -> 대부분은 soft margin\n",
        "svm = SVC(kernel='linear', C=1e10) # linear 커널 사용\n",
        "svm.fit(train_x, train_y)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_x의 개수 : 90\n",
            "train_y의 개수 : 90\n",
            "test_x의 개수 : 10\n",
            "test_y의 개수 : 10\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=10000000000.0, kernel='linear')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kbSRMY9f7UOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8Ez7hULOckE"
      },
      "source": [
        "<h2>SVM 모델을 이용한 평가</h2>\n",
        "\n",
        "<pre>\n",
        "<b>1. svm.predict(data) 함수를 사용하여 SVM 모델을 이용하여 평가</b>\n",
        "  \n",
        "  svm.predict(data)\n",
        "  args\n",
        "    data : indexing 된 문장들이 있는 리스트\n",
        "  return : \n",
        "    입력 문장들에 대한 모델의 출력 라벨 리스트\n",
        "    \n",
        "<b>2. 성능 측정</b>\n",
        "  정답 라벨과 모델의 출력 라벨을 비교하여 성능 측정\n",
        "  \n",
        "<b>3. tokenizer.sequences_to_texts(data) 함수를 이용하여 indexing 된 데이터를 단어로 치환</b>\n",
        "\n",
        "  tokenizer.sequences_to_texts(data)\n",
        "  args\n",
        "    data : indexing 된 리스트\n",
        "  return : \n",
        "    단어로 치환된 리스트\n",
        "    \n",
        "  예시)\n",
        "    [38, 93, 239, 240, 241, 242, 53, 11, 243, 72, 94, 244, 245, 126, 246, 247, 73, 74, 248, 127] -> Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
        "    \n",
        "<b>4. 입력 문장에 대한 모델의 출력과 정답 출력</b>\n",
        "\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gONe3GnfGQcu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ef3f7a2-f86b-4e5b-cc84-e62b4654696f"
      },
      "source": [
        "predict = svm.predict(test_x) \n",
        "\n",
        "#predict 한 결과 출력\n",
        "correct_count = 0\n",
        "for index in range(len(predict)):\n",
        "  #정답과 predict 한 게 맞냐, 몇개 맞았는지 count\n",
        "  if(test_y[index] == predict[index]):\n",
        "    correct_count += 1\n",
        "    \n",
        "## 이거 확률 구하는 거 시험에 나올 듯??\n",
        "# acuuracy 측정    \n",
        "accuracy = 100.0*correct_count/len(test_y)\n",
        "# precision은 ham이라고 내뱉은 것중에 맞은 것\n",
        "# recall은 원래 ham 정답인 것 중에 맞은 것\n",
        "# 원래 accuracy만 하면 안됨\n",
        "# 왜 precision, recall 구해야함?, label이 언발란스하기땜에 클래스별로 해줘야함\n",
        "# 클래스별로 fr이 나와야하고 그거의 평균이 매크로평균?? \n",
        "\n",
        "\n",
        "print(\"Accuracy: \" + str(accuracy))\n",
        "\n",
        "index2label = {0:\"spam\", 1:\"ham\"}\n",
        "\n",
        "# 숫자를 언어로 다시 바꿈\n",
        "test_x_word = tokenizer.sequences_to_texts(test_x)\n",
        "\n",
        "for index in range(len(test_x_word)):\n",
        "  print()\n",
        "  print(\"문장 : \", test_x_word[index])\n",
        "  print(\"정답 : \", index2label[test_y[index]])\n",
        "  print(\"모델 출력 : \", index2label[predict[index]])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 60.0\n",
            "\n",
            "문장 :  yeah do don‘t stand close tho you‘ll catch something\n",
            "정답 :  ham\n",
            "모델 출력 :  ham\n",
            "\n",
            "문장 :  sorry pain ok meet another night spent late afternoon casualty means done stuff42moro includes time sheets that sorry\n",
            "정답 :  ham\n",
            "모델 출력 :  ham\n",
            "\n",
            "문장 :  smile pleasure smile pain smile trouble pours like rain smile sum1 hurts u smile becoz someone still loves see u smiling\n",
            "정답 :  ham\n",
            "모델 출력 :  spam\n",
            "\n",
            "문장 :  please call customer service representative 0800 169 6031 10am 9pm guaranteed ￡1000 cash ￡5000 prize\n",
            "정답 :  spam\n",
            "모델 출력 :  ham\n",
            "\n",
            "문장 :  havent planning buy later check already lido got 530 show e afternoon u finish work already\n",
            "정답 :  ham\n",
            "모델 출력 :  spam\n",
            "\n",
            "문장 :  free ringtone waiting collected simply text password mix 85069 verify get usher britney fml po box 5249 mk17 92h 450ppw 16\n",
            "정답 :  spam\n",
            "모델 출력 :  spam\n",
            "\n",
            "문장 :  watching telugu movie wat abt u\n",
            "정답 :  ham\n",
            "모델 출력 :  ham\n",
            "\n",
            "문장 :  see finish loads loans pay\n",
            "정답 :  ham\n",
            "모델 출력 :  ham\n",
            "\n",
            "문장 :  hi wk ok hols now yes bit run forgot hairdressers appointment four need get home n shower beforehand cause prob u\n",
            "정답 :  ham\n",
            "모델 출력 :  spam\n",
            "\n",
            "문장 :  see cup coffee animation\n",
            "정답 :  ham\n",
            "모델 출력 :  ham\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "nltk.download(\"stopwords\") 를 통해 불용어를 다운 받고 불용어를 제거한 말뭉치들을 모을 리스트 corpus를 생성했다.\n",
        "for 문을 통해 x_data에서 불용어를 다 제거했는데 이때 불용어 리스트를 출력하면 불용어들이 다 소문자로 출력이 돼 정확도를 높이기 위해 불용어를 제거하기 전 x_data 말뭉치들을 다 소문자화한 후 불용어를 제거했다.\n",
        "그 결과 소문자화하기 전 정확도는 50%가 나왔는데 소문자화한 후 정확도는 60%로 올라갔다."
      ],
      "metadata": {
        "id": "reLBnNI6GsOk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B8lbk5647WDD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}